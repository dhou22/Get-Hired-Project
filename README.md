# Get Hired Project - AI Recruitement Assistant
-----
<img width="1024" height="559" alt="image" src="https://github.com/user-attachments/assets/21b6f1a9-2a72-422e-9b45-c56c532484d6" />

-----

A machine learning project that leverages HuggingFace transformers and Weaviate vector database to create semantic embeddings of resumes for intelligent matching and retrieval.

# 1. Overview

This project implements a resume processing pipeline that:
- Generates semantic embeddings from resume text using HuggingFace models
- Stores embeddings in Weaviate vector database for efficient similarity search
- Enables semantic search and matching of resumes based on job requirements

## üöÄ Features

- **Semantic Understanding**: Uses transformer-based models to capture deep semantic meaning from resume text
- **Vector Storage**: Efficient storage and retrieval using Weaviate's vector database
- **Similarity Search**: Find best-matching resumes based on job descriptions or requirements
- **Scalable Architecture**: Built to handle large volumes of resume data

## üõ†Ô∏è Technologies Used

- **Python**: Core programming language
- **HuggingFace Transformers**: Pre-trained models for text embeddings
- **Weaviate**: Vector database for storing and querying embeddings
- **Jupyter Notebook**: Interactive development environment

-----
# 2. Solution architecture 
<img width="1491" height="691" alt="image" src="https://github.com/user-attachments/assets/de5047b5-64aa-4c8b-8500-27e1b64dab7e" />

-----
# 3. Dataset Overview 
-----
<img width="1388" height="365" alt="image" src="https://github.com/user-attachments/assets/4289ca59-7ff7-42af-8041-841c0682528d" />
<br>

<img width="555" height="214" alt="image" src="https://github.com/user-attachments/assets/8aff3f74-3dce-459f-98fb-547e9c1b5f39" />
<br>

<img width="1157" height="655" alt="image" src="https://github.com/user-attachments/assets/5e39cdab-a987-43d8-83b6-41194f8697f4" />

-----
# 4. Resume Preprocessing Pipeline
-----

**Pipeline Steps:**
- **Lowercase conversion** ‚Üí Ensures consistency (e.g., "Python" = "python")
- **Remove punctuation, numbers & special chars** ‚Üí Eliminates noise
- **Tokenization** ‚Üí Splits text into individual words/tokens
- **Stopword removal** ‚Üí Removes common words with little semantic value
- **Lemmatization** ‚Üí Converts words to base form (e.g., "running" ‚Üí "run")
- **Rejoin tokens** ‚Üí Reconstructs clean text for embedding models
<br>

<img width="1145" height="637" alt="image" src="https://github.com/user-attachments/assets/5f9e0afa-3de2-4902-a7a3-4a9273101a20" />
<br>

## üì¶ Stored processed dataset
<img width="880" height="291" alt="image" src="https://github.com/user-attachments/assets/2ef0052a-71bc-4687-9ad4-54685e91ffae" />

------
# 5. Resumes Embedding with Word2VC models 
-----
**1. CBOW (Continuous Bag of Words) predicts target words from context. It's particularly good at:**
- Faster training than Skip-gram
- Better performance with frequent words
- Smoothing over distributional information
- Capturing syntactic patterns effectively

<br> 

**2. Skip-gram (Word2Vec variant) predicts context words from a target word. It excels at:**
- Capturing semantic relationships in smaller datasets
- Learning representations for rare words effectively
- Creating dense vector representations where similar words have similar vectors
<img width="1400" height="725" alt="image" src="https://github.com/user-attachments/assets/9bb9495c-57da-42bc-ba09-3d55ed29c4d2" />

**3. Testing both models**

<img width="618" height="400" alt="image" src="https://github.com/user-attachments/assets/1f8adc68-ee8e-4b76-981e-8a51b4a14d8a" />

**4. Cbow Vs Skip-gram Benchmarking**
<img width="1913" height="851" alt="image" src="https://github.com/user-attachments/assets/ac7876dd-c5d0-46e2-9d2a-5927d763468d" />
<br>

-------
# 6. Resume Embedding with all-MiniLM-L6-v2 model
-------
Load a pre-trained embedding model from HuggingFace.
We use 'all-MiniLM-L6-v2' - a lightweight but powerful model for semantic similarity.
Alternative models:
- 'all-mpnet-base-v2' (higher quality, slower)
- 'paraphrase-multilingual-MiniLM-L12-v2' (multilingual support)

<img width="1409" height="435" alt="image" src="https://github.com/user-attachments/assets/4526d277-13a1-41b1-b8c8-21621c696094" />
<br>

------
# 7. Weaviate cloud integration 
-------
<img width="825" height="346" alt="image" src="https://github.com/user-attachments/assets/72e6c788-8f29-4336-a0ba-faf1a8759929" />
<br>

**1. Data structure shcema in weaviate**
**Schema Purpose**
- Defines the data structure for storing resume information in Weaviate
- Combines vector embeddings with structured metadata for hybrid search capabilities

**Configure.Vectorizer.none()**
- Disables Weaviate's built-in vectorization
- You provide your own embeddings generated by sentence-transformers

**Property Data Types**
- `DataType.TEXT`: Single string values (resume_id, category, text fields)
- `DataType.TEXT_ARRAY`: Multiple string values (skills list)
- `DataType.NUMBER`: Numeric values (experience_years)
- `DataType.DATE`: Timestamp values (processing_date)

**Schema Structure**
- Each Property defines a field name, data type, and description
- Enforces consistent data format across all resume entries
- Enables filtering by metadata (e.g., filter by category or experience level) while performing semantic search

<img width="1890" height="906" alt="Capture d'√©cran 2025-10-04 221249" src="https://github.com/user-attachments/assets/fb16f46f-1147-4ea7-9619-e3f67c71a8ce" />

**2. Process and Upload Resumes to Weaviate**

**process_and_upload_resumes()**
- Main function that transforms raw resume data into vector embeddings and uploads to Weaviate
- Processes resumes in configurable batches to optimize memory usage and network efficiency

**Batch Processing**
- `batch_size=100`: Processes 100 resumes at a time to avoid memory overload
- Iterates through DataFrame in chunks using `range(0, total_resumes, batch_size)`
- Reduces risk of complete failure by isolating errors to individual batches

**Data Transformation Pipeline**
1. Extract text fields from DataFrame (category, original text, preprocessed text)
2. Generate vector embedding using sentence-transformers model
3. Extract structured metadata (skills, experience, education) using helper functions
4. Create unique UUID for each resume
5. Combine all data into structured object matching Weaviate schema

**model.encode()**
- Converts preprocessed text into dense vector embedding
- Output is numerical array representing semantic meaning of resume

**collection.batch.dynamic()**
- Weaviate's batch insertion context manager for efficient bulk uploads
- Automatically handles batch optimization and network requests
- `add_object()`: Adds each resume with both properties (structured data) and vector (embedding)

**Error Handling**
- Try-except blocks at row and batch levels prevent single failures from stopping entire process
- Tracks success/failure counts for transparency
- Continues processing even when individual resumes fail

**Key Features**
- Text truncation (`[:5000]`) prevents extremely long resumes from causing issues
- RFC 3339 datetime format ensures compatibility with Weaviate's DATE type
- Returns statistics for monitoring pipeline performance

<img width="1901" height="894" alt="Capture d'√©cran 2025-10-04 232655" src="https://github.com/user-attachments/assets/188a2d16-d39c-434c-ab59-690d94240966" />

------
# 8. Query Test - Natural laguage queries
-------

<img width="732" height="606" alt="image" src="https://github.com/user-attachments/assets/44b12d74-5fc5-4e3e-9767-9a37d9aa5792" />

------
# 9. Benchmark between the 3 models 
-----
<img width="1135" height="676" alt="Capture d'√©cran 2025-10-05 103102" src="https://github.com/user-attachments/assets/3e5a5d9b-f6ca-4c65-9db1-30952e3670f8" />

üí° KEY INSIGHTS:
CBOW (Continuous Bag of Words): <br>
  ‚úÖ Fast inference speed <br>
  ‚úÖ Compact embeddings (100D) <br>
  ‚úÖ Good for frequent words <br>
  ‚ö†Ô∏è  Limited to training vocabulary <br>
  ‚ö†Ô∏è  Less effective for rare words <br>


Skip-gram: <br>
  ‚úÖ Better semantic relationships <br>
  ‚úÖ Works well with rare words <br>
  ‚úÖ Captures fine-grained meanings <br>
  ‚ö†Ô∏è  Slightly slower than CBOW <br>
  ‚ö†Ô∏è  Limited to training vocabulary <br>


Sentence Transformer (all-MiniLM-L6-v2): <br>
  ‚úÖ Handles ANY word (no OOV issues) <br>
  ‚úÖ Pre-trained on massive datasets <br>
  ‚úÖ Higher dimensional (384D = more info) <br>
  ‚úÖ Better for sentence-level tasks <br>
  ‚ö†Ô∏è  Slower inference <br>
  ‚ö†Ô∏è  Larger model size <br>


Use CBOW/Skip-gram when:
  - You need maximum speed
  - Working with domain-specific vocabulary
  - Training on custom corpus
  - Memory/size is constrained


Use Sentence Transformer when:
  - Need robust OOV handling
  - Working with sentences/phrases
  - Want state-of-the-art quality
  - Inference speed is acceptable
  - Need transfer learning capabilities


## üì¶ Installation

### Prerequisites
- Python 3.8+
- Jupyter Notebook
- Docker (for Weaviate)

### Setup

1. Clone the repository:
```bash
git clone https://github.com/dhou22/Get-Hired-Project.git
cd Get-Hired-Project
```

2. Install required packages:
```bash
pip install -r requirements.txt
```

Or install individually:
```bash
pip install transformers torch weaviate-client sentence-transformers pandas numpy
```

3. Start Weaviate instance:
```bash
docker run -d \
  -p 8080:8080 \
  -e AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true \
  -e PERSISTENCE_DATA_PATH='/var/lib/weaviate' \
  semitechnologies/weaviate:latest
```

## üìì Usage

### Running the Notebook

1. Launch Jupyter Notebook:
```bash
jupyter notebook resume-embedding-huggingface-weaviate-storage.ipynb
```

2. Follow the notebook cells sequentially to:
   - Load and preprocess resume data
   - Generate embeddings using HuggingFace models
   - Store embeddings in Weaviate
   - Query and retrieve similar resumes

### Basic Example

```python
from transformers import AutoTokenizer, AutoModel
import weaviate

# Initialize HuggingFace model
model_name = "sentence-transformers/all-MiniLM-L6-v2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# Connect to Weaviate
client = weaviate.Client("http://localhost:8080")

# Generate embedding for resume text
resume_text = "Your resume text here..."
# ... (embedding generation code)

# Store in Weaviate
# ... (storage code)

# Query for similar resumes
query = "Software engineer with Python experience"
# ... (query code)
```


## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## üìù License

This project is open source and available under the MIT License.

## üë§ Author

**dhou22**
- GitHub: [@dhou22](https://github.com/dhou22)

## üôè Acknowledgments

- HuggingFace for transformer models
- Weaviate for vector database technology
- The open-source community

## üìß Contact

For questions or feedback, please open an issue on GitHub.

---

‚≠ê If you find this project helpful, please consider giving it a star!
